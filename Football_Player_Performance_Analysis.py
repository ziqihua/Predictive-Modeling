# -*- coding: utf-8 -*-
"""545 Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lnv3XNHV6Q8sp25Ss9tpQwgqQ5Q6lvOv

# Football Player Performance Analysis
Our project aims to predict a football player's overall rating based on their skill attributes and physical characteristics. By analyzing these metrics, we can identify which attributes are most predictive of a player’s performance. This project could offer valuable insights for football clubs and scouts to identify promising players based on skills, helping streamline recruitment and training strategies.
The analysis will also be useful for fans and fantasy football participants who want to better understand player capabilities, providing a unique perspective on player metrics beyond popular statistics.
"""

# All Imports Here:
from google.colab import drive
import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import RandomizedSearchCV
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt

"""## Part 1: Data Wrangling

### Loading relevant tables into dataframes
"""

# Mount Google Drive
drive.mount('/content/drive')

db_path = '/content/drive/MyDrive/database.sqlite'

# Connect to the SQLite database
conn = sqlite3.connect(db_path)

# Load data from a table into a DataFrame
player_attributes = 'Player_Attributes'
player = 'Player'
player_attributes_df = pd.read_sql_query(f"SELECT * FROM {player_attributes}", conn)
player_df = pd.read_sql_query(f"SELECT * FROM {player}", conn)

# Close the connection
conn.close()

# Display the Player
player_df.head()

# Display the Player_Attributes
player_attributes_df.head()

"""### Merging table and data wrangling"""

from datetime import datetime

# Drop missing data
player_df = player_df.dropna()
player_attributes_df = player_attributes_df.dropna()

# Add player's age (current year - year of birth)
player_df['age'] = player_df['birthday'].apply(lambda x: datetime.now().year - datetime.strptime(x, "%Y-%m-%d %H:%M:%S").year)

# Merge DataFrames on player_api_id
player_processed_df = pd.merge(player_df, player_attributes_df, on='player_api_id')

# Drop irrelevant columns
columns_to_drop = ['id_x', 'id_y', 'date', 'player_name', 'birthday', 'player_fifa_api_id_x', 'player_fifa_api_id_y', 'player_api_id']
player_processed_df = player_processed_df.drop(columns=columns_to_drop, errors='ignore')

## Now we need to encode 'preferred_foot', 'attacking_work_rate', 'defensive_work_rate'
# Encode 'preferred_foot' as binary
player_processed_df['preferred_foot'] = player_processed_df['preferred_foot'].map({'right': 1, 'left': 0})

# Clean invalid values in 'attacking_work_rate' and 'defensive_work_rate'
valid_work_rate = ['high', 'medium', 'low']

# Replace invalid values with NaN and drop rows with NaN
player_processed_df['attacking_work_rate'] = player_processed_df['attacking_work_rate'].apply(lambda x: x if x in valid_work_rate else None)
player_processed_df['defensive_work_rate'] = player_processed_df['defensive_work_rate'].apply(lambda x: x if x in valid_work_rate else None)

# Drop rows with invalid or missing values in 'attacking_work_rate' and 'defensive_work_rate'
player_processed_df = player_processed_df.dropna(subset=['attacking_work_rate', 'defensive_work_rate'])

# Encode 'attacking_work_rate' and 'defensive_work_rate' as ordinal values
work_rate_mapping = {'low': 0, 'medium': 1, 'high': 2}
player_processed_df['attacking_work_rate'] = player_processed_df['attacking_work_rate'].map(work_rate_mapping)
player_processed_df['defensive_work_rate'] = player_processed_df['defensive_work_rate'].map(work_rate_mapping)

# Verify the processed data
player_processed_df

"""We can see that after preprocessing, we have over 170k rows of data, satisfying our project requirement

## Part 2: EDA

### Distribution of Overall Ratings
"""

plt.figure(figsize=(10, 6))
player_processed_df['overall_rating'].plot(kind='hist', bins=20, edgecolor='black', alpha=0.7)
plt.title('Distribution of Overall Ratings', fontsize=14)
plt.xlabel('Overall Rating', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.grid(axis='y', alpha=0.75)
plt.show()

""" The ratings exhibit a normal distribution, with the majority of players clustered between 60 and 75. The peak frequency lies around 70, indicating that most players in the dataset are of average skill level. Few players have ratings below 50 or above 85, suggesting that exceptionally low or high performers are rare. This finding aligns with the general assumption that players' skill levels tend to center around a typical range, with outliers being less common."""

import seaborn as sns

# Correlation heatmap
plt.figure(figsize=(12, 8))
correlation_matrix = player_processed_df.corr()
sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, annot=False, cbar=True)
plt.title('Correlation Heatmap of Player Attributes', fontsize=14)
plt.show()

"""**Strong Positive Correlation Between Ratings and Skills:** Attributes like reactions show strong positive correlations with overall_rating, indicating their importance in determining player performance.

**Goalkeeping Attributes Are Clustered:** Attributes like gk_diving, gk_handling, and gk_positioning are highly correlated with each other but show little correlation with field player attributes, suggesting a distinct skill set for goalkeepers.

**Age & preferred foot Have Limited Impact:** Physical attributes like age and preferred foot show weak correlations with most skill-related attributes and overall rating, indicating that these are less critical in assessing player performance.
"""

# Identify the top 10 attributes correlated with overall rating
correlation_with_overall = correlation_matrix['overall_rating'].sort_values(ascending=False)
top_10_attributes = correlation_with_overall[1:11]  # Exclude self-correlation

# Bar plot for top 10 correlations
top_10_attributes.plot(kind='bar', figsize=(10, 6), color='skyblue', edgecolor='black')
plt.title('Top 10 Attributes Correlated with Overall Rating', fontsize=14)
plt.ylabel('Correlation', fontsize=12)
plt.xlabel('Attributes', fontsize=12)
plt.grid(axis='y', alpha=0.75)
plt.show()

"""## Part 3: Baseline Model - Linear Regression

### Building the model and evaluate performance
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Prepare the data
# Exclude the 'overall_rating' (target) and 'potential' (irrelevant)
X = player_processed_df.drop(columns=['overall_rating', 'potential'])
y = player_processed_df['overall_rating']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error (MSE):", mse)
print("R-squared (R2):", r2)

"""### Hypotheis Test
**Null Hypothesis (H₀):** The overall_rating is independent of all the attributes; any observed relationship is due to random chance.

**Warning: do not rerun this cell, it takes 10+ minutes to finish**
"""

import numpy as np

# Permutation test
n_permutations = 1000
permuted_r2s = []

for _ in range(n_permutations):
    # Shuffle the target variable (overall_rating)
    y_permuted = np.random.permutation(y)

    # Fit the model with shuffled target
    model.fit(X, y_permuted)

    # Compute R^2 for the shuffled data
    permuted_r2s.append(r2_score(y_permuted, model.predict(X)))

# Compute p-value
permuted_r2s = np.array(permuted_r2s)
p_value = np.mean(permuted_r2s >= r2)

print("Observed R^2:", r2)
print("P-value from permutation test:", p_value)

alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: Attributes significantly explain overall_rating.")
else:
    print("Fail to reject the null hypothesis: Attributes do not significantly explain overall_rating.")

"""## Part 4: Random Forest Model - Predict 'overall_rating'

### Baseline RF Model
"""

# Basic Data Preparation
# Remove goalkeeper features as they're specialized skills
target = 'overall_rating'
gk_features = ['gk_diving', 'gk_handling', 'gk_kicking', 'gk_positioning', 'gk_reflexes']
features = [col for col in player_processed_df.columns if col not in gk_features + [target]]

X = player_processed_df[features]
y = player_processed_df[target]

# Split the data consistently for fair comparison
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Baseline Random Forest Model
def train_baseline_model():
    print("\n=== Baseline Random Forest Model ===")

    # Train basic random forest
    rf_baseline = RandomForestRegressor(random_state=42)
    rf_baseline.fit(X_train_scaled, y_train)

    # Make predictions
    baseline_pred = rf_baseline.predict(X_test_scaled)

    # Calculate metrics
    baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))
    baseline_r2 = r2_score(y_test, baseline_pred)
    baseline_mae = mean_absolute_error(y_test, baseline_pred)

    print(f"Baseline Model Performance:")
    print(f"RMSE: {baseline_rmse:.2f}")
    print(f"R2 Score: {baseline_r2:.2f}")
    print(f"MAE: {baseline_mae:.2f}")

    return {'rmse': baseline_rmse, 'r2': baseline_r2, 'mae': baseline_mae}

baseline_scores = train_baseline_model()

"""### Hyperparameter Tuning"""

# Hyperparameter Tuning of Random Forest
def train_tuned_rf():
    print("\n=== Tuned Random Forest Model ===")

    # Define parameter grid for tuning
    param_dist = {
        'n_estimators': [50, 100, 120],
        'max_depth': [5, 10, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt'],
        'bootstrap': [True]
    }

    # Initialize RandomizedSearchCV
    rf_random = RandomizedSearchCV(
        estimator=RandomForestRegressor(random_state=42),
        param_distributions=param_dist,
        n_iter=20,
        cv=3,
        verbose=1,
        random_state=42,
        n_jobs=2,
        scoring='neg_root_mean_squared_error'
    )

    # Fit the random search model
    rf_random.fit(X_train_scaled, y_train)

    # Make predictions with best model
    tuned_pred = rf_random.predict(X_test_scaled)

    # Calculate metrics
    tuned_rmse = np.sqrt(mean_squared_error(y_test, tuned_pred))
    tuned_r2 = r2_score(y_test, tuned_pred)
    tuned_mae = mean_absolute_error(y_test, tuned_pred)

    print(f"\nTuned Model Performance:")
    print(f"RMSE: {tuned_rmse:.2f}")
    print(f"R2 Score: {tuned_r2:.2f}")
    print(f"MAE: {tuned_mae:.2f}")
    print("Best Parameters:", rf_random.best_params_)

    return {'rmse': tuned_rmse, 'r2': tuned_r2, 'mae': tuned_mae, 'best_params': rf_random.best_params_}

tuned_scores = train_tuned_rf()

"""### PCA Analysis"""

# Step 1: Initial PCA fitting
print("\n=== PCA with Tuned Random Forest Model ===")

# Determine optimal number of components
pca = PCA()
pca.fit(X_train_scaled)

# Find number of components for 95% variance
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)
n_components = np.argmax(cumulative_variance >= 0.95) + 1
print(f"Number of components needed for 95% variance: {n_components}")

# Step 2: Create and fit pipeline with PCA and tuned RF
pipeline = Pipeline([
    ('pca', PCA(n_components=n_components)),
    ('rf', RandomForestRegressor(**tuned_scores['best_params'], random_state=42))
])

pipeline.fit(X_train_scaled, y_train)

# Step 3: Make predictions and calculate metrics
pca_pred = pipeline.predict(X_test_scaled)
pca_rmse = np.sqrt(mean_squared_error(y_test, pca_pred))
pca_r2 = r2_score(y_test, pca_pred)
pca_mae = mean_absolute_error(y_test, pca_pred)

print(f"\nPCA Model Performance:")
print(f"RMSE: {pca_rmse:.2f}")
print(f"R2 Score: {pca_r2:.2f}")
print(f"MAE: {pca_mae:.2f}")

# Step 4: Analyze and visualize variance explained
print("\nVariance Explained by Components:")
for i, var in enumerate(explained_variance_ratio):
    print(f"Component {i+1:2d}: {var*100:6.2f}% of variance")
    print(f"Cumulative : {cumulative_variance[i]*100:6.2f}%")
    if cumulative_variance[i] > 0.99:
        break

# Create variance plot
plt.figure(figsize=(12, 6))
plt.plot(range(1, len(explained_variance_ratio) + 1),
         cumulative_variance, 'bo-', label='Cumulative')
plt.plot(range(1, len(explained_variance_ratio) + 1),
         explained_variance_ratio, 'ro-', label='Individual')
plt.xlabel('Number of Components')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance vs Number of Components')
plt.axhline(y=0.95, color='g', linestyle='--', label='95% Threshold')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()

# Step 5: Analyze component composition
print("\n=== Component Composition Analysis ===")
components_df = pd.DataFrame(
    pca.components_,
    columns=X_train.columns
)

# Show composition of significant components
print(f"\nAnalyzing {n_components} components that explain 95% of variance:")

# Loop through components and show top features
for i in range(5):
    weights = pd.Series(
        abs(components_df.iloc[i]),
        index=X_train.columns
    )
    top_features = weights.nlargest(5)

    print(f"\nComponent {i+1} Major Contributors:")
    for feat, weight in top_features.items():
        print(f"{feat:20s}: {weight:8.3f}")

    # Create bar plot for this component
    plt.figure(figsize=(12, 5))
    ax = top_features.plot(kind='bar')
    plt.title(f'Top Features in Component {i+1}', fontsize=14, pad=20)
    plt.xlabel('Features', fontsize=12)
    plt.ylabel('Absolute Coefficient Value', fontsize=12)
    plt.xticks(rotation=45, ha='right')

    # Add value labels on bars
    for i, v in enumerate(top_features):
        ax.text(i, v, f'{v:.3f}', ha='center', va='bottom')

    plt.tight_layout()
    plt.show()

# Step 6: Correlation Analysis
print("\n=== Correlation Analysis ===")

# After PCA correlations
X_pca = pipeline.named_steps['pca'].transform(X_train_scaled)
pca_data = pd.DataFrame(
    X_pca[:, :n_components],
    columns=[f'PC{i+1}' for i in range(n_components)]
)
plt.figure(figsize=(8, 6))
sns.heatmap(pca_data.corr(), cmap='RdBu', center=0)
plt.title('Component Correlations After PCA')
plt.tight_layout()
plt.show()

# Step 7: Print dimensionality reduction summary
print("\n=== Dimensionality Reduction Summary ===")
print(f"Original features: {X_train.shape[1]}")
print(f"Components kept: {n_components}")
print(f"Feature reduction: {X_train.shape[1] - n_components} features removed")
print(f"Information retained: 95% of original variance")

"""### Model Improvements"""

print("\n=== Model Improvements ===")
print("Tuning Improvements:")
r2_improvement_from_linear = ((tuned_scores['r2'] - r2) / r2) * 100
tuning_rmse_improvement = ((baseline_scores['rmse'] - tuned_scores['rmse']) / baseline_scores['rmse']) * 100
tuning_r2_improvement = ((tuned_scores['r2'] - baseline_scores['r2']) / baseline_scores['r2']) * 100
print(f"RMSE Improvement by Tuning: {tuning_rmse_improvement:.2f}%")
print(f"R2 Score Improvement from Linear Regression: {r2_improvement_from_linear:.2f}%")
print(f"R2 Score Improvement by Tuning: {tuning_r2_improvement:.2f}%")

print("\nPCA Improvements (compared to tuned model):")
pca_rmse_improvement = ((tuned_scores['rmse'] - pca_rmse) / tuned_scores['rmse']) * 100
pca_r2_improvement = ((pca_r2 - tuned_scores['r2']) / tuned_scores['r2']) * 100
print(f"RMSE Improvement after PCA: {pca_rmse_improvement:.2f}%")
print(f"R2 Score Improvement after PCA: {pca_r2_improvement:.2f}%")

reflection = """
Reflections:
While we might be able to combine some attributes (like speed and acceleration) into a single "mobility" score, other attributes might be uniquely important on their own.

In soccer, being excellent at one specific skill can be more valuable than being average at multiple skills.

The technical reasons for the decreased performance include:

1. Information Loss Despite 95% Variance Retention
While we kept 95% of the statistical variance, the 5% we lost might contain crucial subtle differences between players.
For example, the difference between an 85 and 90 shooting rating might be more meaningful than the difference between 60 and 70.

2. Feature Interactions
Soccer player attributes often have non-linear relationships that PCA, being a linear transformation, might not capture well.

This experience teaches us an important lesson about dimensionality reduction:
sometimes maintaining the original feature space is better for prediction, even if it means dealing with more dimensions.
In domains like sports, where individual skills matter greatly and combine in complex ways, direct feature use might be more effective than dimensional reduction.
"""
print(reflection)

"""## Part 5: XGBoost - Predict 'overall_rating' & Analysis with SHAP

"""

import xgboost as xgb
import shap

sns.set(style='whitegrid')

# 2. Define Features and Target
# Assuming 'player_processed_df' is your preprocessed DataFrame
# Replace 'player_processed_df' with your actual DataFrame variable if different

# Check if 'player_processed_df' exists
try:
    player_processed_df
except NameError:
    raise Exception("DataFrame 'player_processed_df' is not defined. Please ensure your data is loaded and preprocessed.")

# Define target variable
target = 'overall_rating'

# Define feature set by excluding the target and any irrelevant columns
# Adjust 'columns_to_exclude' based on your dataset
columns_to_exclude = ['potential']  # Add other columns to exclude if necessary
features = [col for col in player_processed_df.columns if col not in columns_to_exclude + [target]]

X = player_processed_df[features]
y = player_processed_df[target]

print(f"Number of features: {X.shape[1]}")
print(f"Sample features: {features[:5]}")  # Display first 5 feature names

# 3. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"Training samples: {X_train.shape[0]}")
print(f"Testing samples: {X_test.shape[0]}")

# 4. Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Train XGBoost Model
# Initialize XGBoost Regressor with hyperparameters
xgb_reg = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=100,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

# Train the model
print("\nTraining XGBoost Regressor...")
xgb_reg.fit(X_train_scaled, y_train, eval_set=[(X_test_scaled, y_test)], verbose=True)

# 6. Evaluate the Model
# Make predictions on the test set
y_pred_xgb = xgb_reg.predict(X_test_scaled)

# Calculate evaluation metrics
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mse_xgb)

print("\n=== XGBoost Model Performance ===")
print(f"RMSE: {rmse_xgb:.2f}")
print(f"MAE: {mae_xgb:.2f}")
print(f"R²: {r2_xgb:.2f}")

# 7. Apply SHAP for Explainability
# Initialize SHAP Explainer for XGBoost
print("\nApplying SHAP for model explainability...")
explainer = shap.Explainer(xgb_reg, X_train_scaled)

# Calculate SHAP values for the test set
shap_values = explainer(X_test_scaled)

# 8. Visualize SHAP Results

# a. SHAP Summary Plot
print("\nGenerating SHAP Summary Plot...")
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values, X_test_scaled, feature_names=features, show=False)
plt.title('SHAP Summary Plot for XGBoost Model', fontsize=16)
plt.tight_layout()
plt.show()

# b. SHAP Feature Importance (Bar Plot)
print("Generating SHAP Feature Importance Bar Plot...")
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values, X_test_scaled, feature_names=features, plot_type='bar', show=False)
plt.title('SHAP Feature Importance Bar Plot for XGBoost Model', fontsize=16)
plt.tight_layout()
plt.show()

# c. SHAP Dependence Plot for Top Feature
# Identify the top feature based on mean absolute SHAP value
mean_shap = np.abs(shap_values.values).mean(axis=0)
top_feature = features[np.argmax(mean_shap)]

print(f"Generating SHAP Dependence Plot for top feature: {top_feature}...")
plt.figure(figsize=(10, 6))
shap.dependence_plot(top_feature, shap_values.values, X_test_scaled, feature_names=features, show=False)
plt.title(f'SHAP Dependence Plot for {top_feature}', fontsize=16)
plt.tight_layout()
plt.show()

# d. SHAP Force Plot for a Single Prediction (Optional)
# Select an instance to visualize
instance_idx = 0  # Change as needed
print(f"Generating SHAP Force Plot for instance index: {instance_idx}...")
shap.force_plot(explainer.expected_value, shap_values.values[instance_idx], X_test_scaled[instance_idx], feature_names=features, matplotlib=True)
plt.show()

"""## Part 6: Neural Network - Predict 'overall_rating'

### Baseline NN Model
"""

import tensorflow as tf
from tensorflow import keras
from keras import layers, models, callbacks
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error


# Basic Data Preparation
# Remove goalkeeper features as they're specialized skills
target = 'overall_rating'
gk_features = ['gk_diving', 'gk_handling', 'gk_kicking', 'gk_positioning', 'gk_reflexes']
features = [col for col in player_processed_df.columns if col not in gk_features + [target]]

X = player_processed_df[features]
y = player_processed_df[target]

# Split the data consistently for fair comparison
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Assuming you have X_train_scaled, X_test_scaled, y_train, and y_test from previous code:
# X_train_scaled, X_test_scaled: scaled feature sets
# y_train, y_test: target arrays

# Define the architecture of the neural network
def build_model(input_dim):
    model = models.Sequential()
    # Input Layer
    model.add(layers.Dense(units=128, activation='relu', input_shape=(input_dim,)))
    model.add(layers.Dropout(rate=0.2))

    # Hidden Layers
    model.add(layers.Dense(units=64, activation='relu'))
    model.add(layers.Dropout(rate=0.2))

    model.add(layers.Dense(units=32, activation='relu'))
    model.add(layers.Dropout(rate=0.1))

    # Output Layer (Regression)
    model.add(layers.Dense(units=1))  # linear activation is default for last layer in regression

    # Compile the model
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# Build the model
input_dim = X_train_scaled.shape[1]
dnn_model = build_model(input_dim)

# Early Stopping to prevent overfitting
early_stopper = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Fit the model
history = dnn_model.fit(
    X_train_scaled,
    y_train,
    validation_split=0.2,
    epochs=10,
    batch_size=512,
    callbacks=[early_stopper],
    verbose=1
)

# Evaluate the model on test set
dnn_pred = dnn_model.predict(X_test_scaled).ravel()

mse_nn = mean_squared_error(y_test, dnn_pred)
mae_nn = mean_absolute_error(y_test, dnn_pred)
r2_nn = r2_score(y_test, dnn_pred)
rmse_nn = mse_nn**0.5

print("=== Neural Network Model Performance ===")
print(f"RMSE: {rmse_nn:.2f}")
print(f"MAE: {mae_nn:.2f}")
print(f"R²: {r2_nn:.2f}")

# Plot training history for loss
import matplotlib.pyplot as plt

# Create a range for epochs excluding the first epoch
total_epochs = len(history.history['loss'])
epochs = range(2, total_epochs + 1)  # Start from epoch 2

# Plot Training Loss excluding the first epoch
plt.figure(figsize=(10, 6))
plt.plot(epochs, history.history['loss'][1:], label='Training Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.title('Training Loss Over Epochs')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Plot Validation Loss excluding the first epoch
plt.figure(figsize=(10, 6))
plt.plot(epochs, history.history['val_loss'][1:], label='Validation Loss', color='orange')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.title('Validation Loss Over Epochs')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

"""### Training NN model after PCA

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

import tensorflow as tf


# Define target variable
target = 'overall_rating'

# Define feature set by excluding the target and any irrelevant columns
# Adjust 'columns_to_exclude' based on your dataset
columns_to_exclude = ['potential']  # Add other columns to exclude if necessary
features = [col for col in player_processed_df.columns if col not in columns_to_exclude + [target]]

X = player_processed_df[features]
y = player_processed_df[target]

# 4. Train-Test Split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 5. Feature Scaling

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 6. Apply PCA

# Initialize PCA to retain 95% of the variance
pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

print(f"Original number of features: {X_train_scaled.shape[1]}")
print(f"Reduced number of features after PCA: {X_train_pca.shape[1]}")

# 7. Build and Train Neural Network

# Define the architecture of the neural network
def build_pca_model(input_dim):
    model = models.Sequential()
    # Input Layer
    model.add(layers.Dense(units=128, activation='relu', input_shape=(input_dim,)))
    model.add(layers.Dropout(rate=0.2))

    # Hidden Layers
    model.add(layers.Dense(units=64, activation='relu'))
    model.add(layers.Dropout(rate=0.2))

    model.add(layers.Dense(units=32, activation='relu'))
    model.add(layers.Dropout(rate=0.1))

    # Output Layer (Regression)
    model.add(layers.Dense(units=1))  # Linear activation for regression

    # Compile the model
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# Build the model
input_dim_pca = X_train_pca.shape[1]
dnn_pca_model = build_pca_model(input_dim_pca)
dnn_pca_model.summary()

# Early Stopping to prevent overfitting
early_stopper_pca = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Train the model
history_pca = dnn_pca_model.fit(
    X_train_pca,
    y_train,
    validation_split=0.2,        # 20% of training data for validation
    epochs=10,                   # Maximum number of epochs
    batch_size=512,              # Batch size
    callbacks=[early_stopper_pca],
    verbose=1
)

# 8. Evaluate the Model

# Make predictions on the test set
dnn_pca_pred = dnn_pca_model.predict(X_test_pca).ravel()

# Calculate evaluation metrics
mse_pca = mean_squared_error(y_test, dnn_pca_pred)
mae_pca = mean_absolute_error(y_test, dnn_pca_pred)
r2_pca = r2_score(y_test, dnn_pca_pred)
rmse_pca = np.sqrt(mse_pca)

print("\n=== Neural Network with PCA Model Performance ===")
print(f"RMSE: {rmse_pca:.2f}")
print(f"MAE: {mae_pca:.2f}")
print(f"R²: {r2_pca:.2f}")

# 9. Visualize Training and Validation Loss

# Determine the number of epochs trained
total_epochs_pca = len(history_pca.history['loss'])
epochs_pca = range(2, total_epochs_pca + 1)  # Start from epoch 2

# Plot Training Loss excluding the first epoch
plt.figure(figsize=(10, 6))
plt.plot(epochs_pca, history_pca.history['loss'][1:], label='Training Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.title('Training Loss Over Epochs (Excluding First Epoch) - PCA')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Plot Validation Loss excluding the first epoch
plt.figure(figsize=(10, 6))
plt.plot(epochs_pca, history_pca.history['val_loss'][1:], label='Validation Loss', color='orange')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.title('Validation Loss Over Epochs (Excluding First Epoch) - PCA')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

"""# Implications & Future Work
**Implications** this analysis highlights how clubs and scouts could focus on specific attributes, such as reactions, which consistently emerged as a key predictor, to identify promising players. It also reinforces the importance of technical skills like dribbling and ball_control for overall performance.

**Challenges**
Data Imbalance: The distribution of player ratings is normal, but there are fewer examples of high- and low-performing players. This imbalance could limit the model's ability to predict extreme cases.

**Potential Future Work**
Role-Specific Analysis: Create separate models for different player positions, as the attributes important for goalkeepers, defenders, and strikers may differ significantly.

"""